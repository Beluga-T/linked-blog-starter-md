# Section 1: Methods Selection Reasoning

## 1.1 Dataset Overview
![[Pasted image 20251030163426.png]]
The Loan Approval Prediction dataset contains 33 features with a significant class imbalance of 76.1% rejected versus 23.9% approved applications. Feature selection is essential for this dataset to reduce model complexity, improve interpretability for regulatory compliance, and address potential multicollinearity among financial variables such as income, debt ratios, and asset values. 
## 1.2 Scikit-learn Method 1: Recursive Feature Elimination (RFE)

Recursive Feature Elimination was selected as the first Scikit-learn method due to its wrapper approach that evaluates feature subsets iteratively through model retraining. This method works exceptionally well with Random Forest as the base estimator, making it particularly suitable for capturing non-linear relationships prevalent in financial data where variables such as debt-to-income ratio interact multiplicatively with income levels. RFE provides explicit feature rankings that enable comprehensive analysis of relative importance across all variables, offering transparency in the feature selection process that is valuable for both model interpretation and business decision-making in loan approval contexts.
![[Pasted image 20251030165224.png|500]]
The mechanism of RFE involves recursively eliminating the least important features based on Random Forest feature importances calculated from mean decrease in impurity across decision trees. Starting with all 33 features, the algorithm trains the Random Forest model, ranks features by their importance scores, removes the weakest feature, and repeats this process until reaching the target of 16 features. This iterative approach ensures that feature interactions are reconsidered at each elimination step, making it particularly suitable for complex loan approval criteria where the importance of one variable (such as employment type) may depend on the presence of others (such as income stability). 
For this analysis, RFE successfully reduced the feature space by 51.5% from 33 to 16 features as shown above.

## 1.3 Scikit-learn Method 2: SelectFromModel

SelectFromModel was chosen as the second Scikit-learn method because it represents an embedded approach that performs feature selection simultaneously with model training, providing a complementary perspective to RFE's wrapper methodology. This method uses Random Forest's intrinsic feature importances, which are calculated during tree construction based on how much each feature reduces Gini impurity across all decision splits. The efficiency advantage is substantial, requiring only a single training pass compared to RFE's 17 iterations, reducing computational cost by approximately 90% while still capturing complex feature interactions inherent in tree-based models.

![[Pasted image 20251030165944.png|500]]

SelectFromModel is particularly well-suited for the loan approval dataset because Random Forest exhibits robustness to the 76.1% to 23.9% class imbalance through its ensemble architecture. The bootstrap sampling mechanism creates natural diversity across the 100 constituent trees, with each tree observing slightly different class distributions due to random sampling with replacement. This ensemble diversity, combined with majority voting across trees, provides inherent resilience against class imbalance compared to single-model approaches. Critically, we addressed the imbalance through evaluation methodology by prioritizing Macro F1-Score, which equally weights both classes (rejected and approved loans) rather than allowing the 76.1% majority class to dominate performance metrics. This metric-level approach ensures that model selection and optimization favor balanced performance across both loan outcomes.

SelectFromModel's feature count is comparable to RFE, but the selection process is fundamentally different, with SelectFromModel prioritizing features that contribute most to impurity reduction in the initial model training rather than through iterative elimination.

## 1.4 Genetic Algorithm

GA explores the feature combination efficiently via evolutionary mechanisms including crossover (combining feature sets from parent solutions), mutation (randomly flipping feature inclusion), and tournament selection (preferring higher-performing feature subsets). Most importantly, i design the fitness function to balance prediction performance, weighted at 80% through Macro F1-Score, with parsimony, weighted at 20% through feature count reduction:

`fitness = 0.8 × Macro_F1 + 0.2 × (1 - n_features/33)`

The implementation used a population size of 80 individuals represented as binary chromosomes, where each bit indicates whether a feature is included (1) or excluded (0), with evolution proceeding for a maximum of 60 generations. 

A crossover rate of 85% promotes solution mixing and rapid propagation of beneficial feature combinations, while a controlled mutation rate of 5% maintains exploration without disrupting well-performing solutions. Tournament selection with size 7 creates strong selection pressure favoring individuals with higher fitness scores. Early stopping after 15 consecutive generations without improvement greater than 0.0001 prevents wasted computation once convergence is reached. Ten independent runs were executed to ensure statistical robustness, with the best-performing run (Run 2) selected based on highest Macro F1-Score. 


---

# Section 2: Quantitative Analysis

## 2.1 Feature Reduction


![[Pasted image 20251030180911.png|450]]

**Table: Feature Reduction Comparison**

| Method                         | Features Selected | Reduction % |
| ------------------------------ | ----------------- | ----------- |
| Baseline (All Features)        | 33                | 0%          |
| RFE                            | 16                | 51.5%       |
| SelectFromModel                | 17                | 48.5%       |
| Genetic Algorithm (Best Run 2) | 8                 | 75.8%       |
Table and graph above presents the number of features selected by each method compared to the 33-feature baseline. RFE achieved a moderate 51.5% reduction by selecting 16 features, while SelectFromModel showed similar conservative reduction at 48.5% with 17 features selected.The GA achieved the highest feature reduction at 75.8%, selecting only 8 features while using merely 24% of the original feature space. This aggressive reduction by GA suggests that the dataset contains significant redundancy, with the majority of features providing only marginal predictive value once the core discriminators are identified.
## 2.2 Performance Comparison

Table 2 presents comprehensive performance metrics for all methods, with Macro F1-Score highlighted as the primary evaluation metric due to the 3.2:1 class imbalance ratio (76.1% rejected vs 23.9% approved loans). Macro F1-Score was prioritized over accuracy because it treats both classes equally by averaging their individual F1-scores, ensuring that model performance on the minority class (approved loans) is not overshadowed by performance on the majority class (rejected loans). This is crucial for fair lending practices where misclassifying approved loans has direct business consequences.

**Table 2: Performance Metrics Comparison**

| Method | Accuracy | Precision (Macro) | Recall (Macro) | F1 (Weighted) | **F1 (Macro)** |
|--------|----------|-------------------|----------------|---------------|----------------|
| Baseline | 0.9305 | 0.9161 | 0.8887 | 0.9293 | **0.9013** |
| RFE | 0.9285 | 0.9105 | 0.8892 | 0.9275 | **0.8991** (-0.24%) |
| SelectFromModel | 0.9260 | 0.9083 | 0.8839 | 0.9249 | **0.8952** (-0.68%) |
| Genetic Algorithm | 0.9353 | 0.9175 | 0.9019 | 0.9346 | **0.9093** (+0.89%) |

The Genetic Algorithm achieved the highest Macro F1-Score of 0.9093, representing a +0.89% improvement over the 33-feature baseline despite using 75.8% fewer features. This result is both statistically and practically significant, demonstrating that intelligent feature selection through evolutionary optimization not only maintains performance but actually exceeds the full-feature model by eliminating noisy, weakly-predictive variables that can introduce overfitting. RFE experienced a modest -0.24% performance decrease compared to baseline with its 16-feature subset, while SelectFromModel showed a -0.68% decrease with 17 features. The fact that GA improved upon baseline performance while RFE and SelectFromModel both experienced minor decreases suggests that GA's fitness-based optimization successfully identified a synergistic feature combination that RFE's greedy elimination and SelectFromModel's importance thresholding did not discover.


![[Pasted image 20251030181043.png|550]]

This graph focuses specifically on Macro metrics (Precision, Recall, F1) which equally weight both loan approval classes, providing a complementary view to Figure 2's weighted metrics. Again, GA's bars are consistently the tallest across all three Macro metric charts, with its Macro F1 bar reaching 0.9093 compared to baseline's 0.9013. This visual reinforcement emphasizes that GA's performance advantage is robust across both weighted and unweighted evaluation schemes, crucial for demonstrating that the model performs well for both the majority class (rejected loans) that dominates weighted metrics and the minority class (approved loans) that has equal importance in Macro metrics.
## 2.3 Per-Class Performance Analysis

Table below breaks down F1-scores for each individual class, revealing how methods perform on rejected loans (majority class, 76.1%) versus approved loans (minority class, 23.9%). The Balance (Δ) column calculates the absolute difference between the two classes' F1-scores, where a smaller delta indicates more balanced performance across classes.

**Table: Per-Class F1-Score Analysis**

| Method | Rejected (0) F1 | Approved (1) F1 | Balance (Δ) |
|--------|-----------------|-----------------|-------------|
| Baseline | 0.9489 | 0.8537 | 0.0952 |
| RFE | 0.9471 | 0.8512 | 0.0959 |
| SelectFromModel | 0.9452 | 0.8452 | 0.1000 |
| Genetic Algorithm | 0.9515 | 0.8671 | **0.0844** |

The Genetic Algorithm achieved the most balanced performance with the smallest class gap (Δ=0.0844), indicating consistent predictions for both rejected loans (F1=0.9515) and approved loans (F1=0.8671). Its approved-loan F1 is 1.34 percentage points higher than the baseline (0.8537) and 1.59 points higher than RFE (0.8512), meaning it better identifies qualified applicants and reduces costly false rejections.

Notably, GA also improved the majority-class F1 (0.9515 vs. baseline 0.9489), showing it enhances both classes rather than trading one off. SelectFromModel had the largest gap (Δ=0.1000), suggesting greater sensitivity to class imbalance. Overall, GA’s aggressive reduction to 8 features not only preserved but improved fairness and accuracy, demonstrating that removing noisy or redundant features can strengthen performance across classes.

---

# Section 3: Qualitative Analysis

## 3.1 Feature Importance and Interpretability

### 3.1.1 Consistently Selected Features

5 features were selected by all three methods, representing robust predictors that transcend selection methodology. These universal features are: `AnnualIncome` `InterestRate` `LengthOfCreditHistory` `totalAssets` `TotalDebtToIncomeRatio`. The 100% agreement across methods indicates these are fundamental discriminators of loan approval regardless of whether features are selected through iterative wrapper methods (RFE), embedded importance scores (SelectFromModel), or evolutionary fitness optimization (GA).

Fourteen additional features were selected by at least two methods, showing strong but method-dependent importance. Some features contribute primarily through interactions with other features rather than independent predictive power, explaining why iterative and evolutionary methods that can discover synergies identify them while single-pass importance ranking does not.

### 3.1.2 Disproportionately Influential Features

![[Pasted image 20251031063813.png|550]]
During exploratory data analysis, we identified RiskScore as a feature exhibiting strong evidence of data leakage, necessitating its exclusion from all feature selection and modeling procedures. 
Four independent indicators confirmed the leakage: first, RiskScore exhibited an extremely high correlation of r=-0.7661 with the target variable LoanApproved, far exceeding the threshold of |r|>0.7 typically indicative of post-decision contamination

**Table: Predictive Power Comparison**

| Feature                | Single-Feature Macro F1 | Status       | Reason                                 |
| ---------------------- | ----------------------- | ------------ | -------------------------------------- |
| **RiskScore**          | **0.9736**              | **EXCLUDED** | **Data leakage (r=-0.77 with target)** |
| TotalDebtToIncomeRatio | 0.7220                  | ✓ Retained   | Legitimate predictor                   |
| CreditScore            | 0.6543                  | ✓ Retained   | Legitimate predictor                   |
| InterestRate           | 0.6213                  | ✓ Retained   | Legitimate predictor                   |
RiskScore's Macro F1 of 0.9736 represents 8% higher performance than the best feature selection method (GA at 0.9093). This impossibly high single-feature performance almost definitive is a proof of data leakage. The variable likely represents a post-hoc risk assessment calculated by the lender after observing applicant characteristics and possibly the approval outcome itself, creating a circular dependency where the feature contains information derived from the prediction target. Including RiskScore would create a model that appears highly accurate during development but would catastrophically fail in production deployment, as this derived variable would not exist at the time of actual loan application processing.

![[Pasted image 20251030185124.png|500]]
All feature selection methods (RFE, SelectFromModel, GA) were executed on the 33-feature dataset after RiskScore exclusion. The original dataset contained 34 numerical features after one-hot encoding, but RiskScore removal reduced this to 33 features for analysis. This exclusion actually strengthens the validity of our results, as the comparative performance of the three methods reflects their ability to identify genuine predictive patterns rather than simply discovering a leaked variable.

## 3.2 Handling of Feature Correlation

### 3.2.1 Correlation Analysis
![[Pasted image 20251031061904.png|560]]

#### Table: High Correlation Feature Pairs (|r| > 0.8)

| Feature 1 | Feature 2 | Correlation (r) | RFE | SelectFromModel | GA |
|-----------|-----------|-----------------|-----|-----------------|-----|
| Age | Experience | 0.983 | Both selected | Neither selected | Neither selected |
| AnnualIncome | MonthlyIncome | 0.990 | Both selected | Both selected | AnnualIncome only |
| TotalAssets | NetWorth | 0.979 | Both selected | Both selected | TotalAssets only |
| BaseInterestRate | InterestRate | 0.835 | Both selected | Both selected | InterestRate only |

4 pairs of features exhibited high correlation with absolute Pearson correlation coefficients exceeding 0.8, indicating substantial multicollinearity. 

These high correlations create redundancy where two features provide largely overlapping information, raising concerns about multicollinearity effects such as unstable coefficient estimates in linear models and inflated variance in predictions. However, tree-based models like Random Forest are generally more robust to multicollinearity than linear models because decision trees evaluate features independently at each split point rather than estimating joint coefficients. 

### 3.2.2 Method Comparison of Correlation Handling

Each feature selection method addressed these correlated pairs differently based on its underlying mechanism. SelectFromModel emerged as the most aggressive in eliminating correlated features, removing four out of eight features involved in high-correlation pairs. GA was nearly as effective, eliminating three of eight. RFE was most conservative, removing only two of eight, suggesting its iterative nature values incremental information even from correlated features that may contribute through subtle interactions with other variables. The impact on performance validates GA's approach: models with fewer correlated features, exemplified by GA's zero high-correlation pairs in its final 8-feature set, demonstrated better generalization performance. 

GA's 0.9093 Macro F1 with zero high-correlation pairs compared to RFE's 0.8991 and SelectFromModel's 0.8952 supports the hypothesis that eliminating redundant features reduces model variance without sacrificing bias, leading to improved generalization.

## 3.3 Categorical Feature Encoding

### 3.3.1 Encoding Strategy

Five categorical features in the original dataset required transformation to numerical representations for use in Random Forest models: EmploymentStatus (3 categories: Employed, Self-Employed, Unemployed), EducationLevel (5 categories: High School, Associate, Bachelor, Master, Doctorate), MaritalStatus (4 categories: Single, Married, Divorced, Widowed), HomeOwnershipStatus (4 categories: Own, Mortgage, Rent, Other), and LoanPurpose (5 categories: Home, Auto, Education, Debt Consolidation, Other). 

Label encoding was applied to all five categorical variables, assigning each category a unique integer. This transformation maintained the original feature count at 33 features (excluding RiskScore), as each categorical variable was converted to a single numerical column rather than expanding into multiple binary indicators.
### 3.3.2 Label Encoding in Tree-Based Models

Label encoding suits Random Forest well because decision trees split on thresholds rather than treating numbers as continuous magnitudes. When evaluating "EmploymentStatus ≤ 1?", the algorithm simply partitions data without assuming EmploymentStatus=0 is meaningfully "less than" EmploymentStatus=2. The ensemble of hundreds of trees with random split points collectively captures categorical relationships despite the artificial ordering imposed by numeric codes.

For nominal variables like LoanPurpose where Home, Auto, and Education have no natural sequence, label encoding could theoretically bias splits. Random Forest compensates through three mechanisms: random feature selection at each node limits over-reliance on any single categorical, the tree ensemble explores diverse split configurations to discover true category relationships, and aggregated feature importances reflect genuine predictive power independent of encoding order.

### 3.3.3 Impact on Feature Selection

I think the label encoding approach had distinct impacts on each feature selection method's ability to identify categorical predictors. None of the three methods selected any categorical features in their final solutions. RFE's 16 features, SelectFromModel's 17 features, and GA's 8 features all consisted entirely of numerical financial variables (income, credit, debt ratios, assets). This universal exclusion of categorical features suggests that financial capacity variables dominate loan approval prediction, with demographics providing minimal incremental value once financial metrics are included.

This pattern likely reflects two underlying factors. First, categorical variables encoded as integers may have lower feature importance scores in Random Forest compared to continuous financial variables that exhibit stronger linear relationships with loan approval. For example, AnnualIncome spans a wide continuous range that enables fine-grained risk stratification, whereas EmploymentStatus with only 3 discrete values provides coarse segmentation. Second, financial variables capture most information that categorical demographics would provide—for instance, EmploymentStatus correlates with income stability, and EducationLevel correlates with earning potential, making them partially redundant once income and credit history are included.

### Section 4: Conclusion

RFE ranks second with a Macro F1 of 0.8991 and a 51.5% feature reduction, offering a balanced trade-off between compactness and caution. Its key strength is transparent feature ranking, which supports interpretability and model governance. However, its iterative training (17 retrains) makes it computationally expensive, and the small −0.24% performance drop vs. baseline shows that greedy elimination does not match GA’s global optimization ability.

SelectFromModel places third with a Macro F1 of 0.8952 and a 48.5% reduction (17 features). Its main advantage is efficiency—only one model fit—making it well-suited for fast experimentation or limited compute environments. But it had the largest performance decrease (−0.68%), and its fixed median-importance threshold may not be ideal for all datasets, potentially requiring tuning despite being positioned as “automatic.”
#### Best Method: Genetic Algorithm

The Genetic Algorithm (Run 2) is the optimal feature selection method based on four key strengths:

GA achieved the highest Macro F1-Score of 0.9093, improving +0.89% over the 33-feature baseline (0.9013) while using only 8 features. This contradicts the expectation that aggressive feature reduction sacrifices performance. RFE and SelectFromModel both decreased performance, demonstrating GA's unique ability to discover synergistic feature combinations through evolutionary optimization. GA reduced features by 75.8% while improving performance. The 8-feature model offers faster inference, reduced data collection burden, simpler maintenance, and better regulatory interpretability. 
